SemEval-2017 Task 7: Detection and Interpretation of English Puns
==================================================================

This archive contains trial and test data, scoring software, and raw
system results for the SemEval-2017 shared task on the detection and
interpretation of English puns.

If you use the data or scorer in your own work, please cite the
following paper, a preprint of which is also included in this archive:

Tristan Miller, Christian F. Hempelmann, and Iryna Gurevych.
SemEval-2017 Task 7: Detection and interpretation of English puns. In
Proceedings of the 11th International Workshop on Semantic Evaluation
(SemEval-2017), pages 59â€“69, August 2017. ISBN 978-1-945626-00-5.

```
@inproceedings{miller2017semeval,
   author       = {Tristan Miller and Christian F. Hempelmann and
                   Iryna Gurevych},
   title        = {{SemEval}-2017 {Task}~7: {Detection} and
                   Interpretation of {English} Puns},
   booktitle    = {Proceedings of the 11th International Workshop on
                   Semantic Evaluation (SemEval-2017)},
   pages        = {59--69},
   month        = aug,
   year         = {2017},
   isbn         = {978-1-945626-00-5},
}
```

Data
----

In the `data/trial` directory, there are six XML files that cover each
of the three subtasks (pun detection, pun location, and pun
interpretation) and pun types (heterographic and homographic) as
described in the task paper and on the
[task web page](http://alt.qcri.org/semeval2017/task7/).  In
combination with the document type definition (DTD) in the file
`puns.dtd`, these XML files should be self-explanatory.

Associated with each XML file is a delimited text file that
illustrates the answer format expected by the scoring software.  These
formats are as follows:

For subtask 1, each line consists of two fields separated by
horizontal whitespace (a single tab or space character).  The first
field is the ID of a text from the XML file.  The second field is `1`
if the text contains a pun, or `0` if the text does not contain a pun.

For subtask 2, each line consists of two fields separated by
horizontal whitespace (a single tab or space character).  The first
field is the ID of a text from the XML file.  The second field is the
ID of the one word in that text which is a pun.

For subtask 3, each line consists of three fields separated by
horizontal whitespace (a single tab or space character).  The first
field is the ID of a pun word.  The second field is a
semicolon-delimited list of WordNet 3.1 sense keys that match one
meaning of the pun.  The third field is a semicolon-delimited list of
WordNet 3.1 sense keys that match the other meaning of the pun.

The order of the two meanings is not significant, nor is the order of
the sense keys within each meaning.  Please refer to the task paper or
visit the task web page for further details on the scoring metrics.

The `data/test` directory contains the test data (six XML files that
cover each of the three subtasks and pun types, and associated
delimited text files providing the answer key).

Scorer
------

The `scorer/src` directory contains the Java source code for the
scoring software and the `scorer/bin` directory contains the compiled
classes.

To (re)compile the scoring software:

```
$ cd scorer
$ javac -d bin src/de/tudarmstadt/ukp/semeval2017/task7/scorer/PunScorer.java
```

To run the scoring software:

```
$ cd scorer/bin
$ java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer
```

Running the scorer without any command-line arguments prints the
following usage instructions:

```
Usage:
        java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer [ -d | -l | -i ] <goldFile> <resultFile> [ <outputFile> ]
```

The first command-line argument is required, and must be either `-d`
(for the detection subtask), `-l` (for the location subtask), or `-i`
(for the interpretation subtask).  The next two arguments are also
required; they specify the location of the gold-standard file and the
system result file, in that order.  The final optional argument
specifies the location of a file to write the output.  If the argument
is omitted, output is written to standard output.

Results
-------

The `results` directory contains the raw system result files submitted
by the task participants or generated by the baseline algorithms, the
scores for which were published in the task paper mentioned above.  A
result file for the maximum polysemy pun location baseline is not
available; its scores were calculated using a special program (not
included in this distribution).  As noted in the task paper, the pun
detection results for ECNU and Fermi use only a subset of the full
test data; these subsets are included in the files
`results/detection/ECNU_het.gold`, `results/detection/ECNU_hom.gold`,
and `results/detection/Fermi_hom.gold`.

Copyright and licensing
-----------------------

See the file `LICENCE.md`.

Version history
---------------

2017-06-21: Initial release of the full archive.

Contact
-------

[Tristan Miller](mailto:miller@ukp.informatik.tu-darmstadt.de)
